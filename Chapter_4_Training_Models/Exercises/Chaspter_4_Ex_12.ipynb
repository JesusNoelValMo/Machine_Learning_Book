{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chaspter_4_Ex_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BaAzw0fWAo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB4zM_zLbHQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBiPzWQZetQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X = mnist['data']\n",
        "X.shape\n",
        "y = mnist['target'].ravel()\n",
        "X_w_bias = np.c_[np.ones([len(X), 1]), X]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JSmipW0gQ3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalize X\n",
        "X_w_bias = X_w_bias /255.0 #To the X go from 0 to 1, instead of 0 to 255\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_w_bias, y, test_size = 0.4, random_state = 2)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG_u0qBhhcFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d20a4d9-f6be-4f7e-f584-80b9d89c9684"
      },
      "source": [
        "#Let's take a validation set \n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 2)\n",
        "n_inputs = X_train.shape[1] #Number of features (all the pixels plus the bias term) \n",
        "n_outputs = len(np.unique(y_train)) #Number of possible classes (0 to 9 )\n",
        "y_train"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1', '0', '5', ..., '1', '8', '3'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7QD3fuOHxE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "#One Hot the classes with a OneHotEncoder in order to have de probabilities of being any of this chances \n",
        "#on the labels \n",
        "#It is important to keep our encoded data as arrays\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
        "y_val_encoded = encoder.fit_transform(y_val.reshape(-1,1)).toarray()\n",
        "y_test_encoded = encoder.fit_transform(y_test.reshape(-1,1)).toarray()\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg_if7W_IwPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b27da69-b176-4f01-917d-a9c62a07f30d"
      },
      "source": [
        "y_train_encoded.shape"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17203, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3COGUceLY8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Softmax(logits):\n",
        "  exps = np.exp(logits)\n",
        "  exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "  return exps / exp_sums\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qv4TcPXh7JI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "0d3cb587-fc71-4e22-de16-bf341894544b"
      },
      "source": [
        "\n",
        "#Lets do the Batch Gradient descent without the softmaxt algorithm\n",
        "eta = 0.1 #Learning rate\n",
        "n_iterations = 2501\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "thetas = np.random.rand(n_inputs, n_outputs) #Thetas for all the features and all the possible classes (all pixels plus bias X 0 to 9 classes)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "  logits = X_train.dot(thetas)\n",
        "  y_proba = Softmax(logits)\n",
        "  loss = -np.mean(np.sum(y_train_encoded * np.log(y_proba + epsilon), axis=1)) #Cross entropy cost function\n",
        "  if iteration % 500 == 0:\n",
        "    print(iteration, loss)\n",
        "  erro = y_proba - y_train_encoded\n",
        "  gradients = 1/m * X_train.T.dot(erro)\n",
        "  thetas = thetas - eta * gradients\n",
        "\n",
        "  "
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5.062183557169024\n",
            "500 0.4772517929622487\n",
            "1000 0.39699479388965025\n",
            "1500 0.3617676820405957\n",
            "2000 0.34050550170449406\n",
            "2500 0.32577036403268544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZGPSGg5W1Ic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "2e4eaecf-65f5-43ea-f2b3-df5dadf374af"
      },
      "source": [
        "thetas\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23867834, 0.52739878, 0.80025867, ..., 0.77965808, 0.10116605,\n",
              "        0.10966146],\n",
              "       [0.90179001, 0.39320626, 0.67858522, ..., 0.78666287, 0.18191937,\n",
              "        0.61427201],\n",
              "       [0.31336247, 0.94377811, 0.74271074, ..., 0.86125228, 0.63717742,\n",
              "        0.62499266],\n",
              "       ...,\n",
              "       [0.62450795, 0.13821028, 0.89058998, ..., 0.71429873, 0.69670821,\n",
              "        0.56683896],\n",
              "       [0.81314645, 0.07637499, 0.2224422 , ..., 0.79962577, 0.90421759,\n",
              "        0.67183655],\n",
              "       [0.52307219, 0.01025581, 0.82452387, ..., 0.06324123, 0.49287149,\n",
              "        0.41399952]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK4tmN0bNOSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f7b4288-5378-4687-e81c-11bd34b210ab"
      },
      "source": [
        "#Let's do predictions for the validation set and check the accuracy score\n",
        "logits = X_val.dot(thetas)\n",
        "y_proba = Softmax(logits)\n",
        "prediction = np.argmax(y_proba, axis=1) #Returns the INDEX with the highest probability\n",
        "y_val_int = [int(string_val) for string_val in y_val ]\n",
        "score_accuracy = np.mean(prediction == y_val_int)\n",
        "prediction\n"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 9, 5, ..., 4, 8, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHPQJDmkdwU7",
        "colab_type": "text"
      },
      "source": [
        "let's add a bit of $\\ell_2$ regularization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGuYEB87dklS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "dbbd9e19-63f0-433d-f59b-0d8397af66a8"
      },
      "source": [
        "#Lets do the Batch Gradient descent without the softmaxt algorithm\n",
        "eta = 0.1 #Learning rate\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1 #Regularization Hyperparameter\n",
        "\n",
        "\n",
        "\n",
        "thetas = np.random.rand(n_inputs, n_outputs) #Thetas for all the features and all the possible classes (all pixels plus bias X 0 to 9 classes)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "  logits = X_train.dot(thetas)\n",
        "  y_proba = Softmax(logits)\n",
        "  X_entropy_loss = -np.mean(np.sum(y_train_encoded * np.log(y_proba + epsilon), axis=1)) #Cross entropy cost function\n",
        "  l2_loss = 1/2 * np.sum(np.square(thetas[1:]))\n",
        "  loss = X_entropy_loss + alpha * l2_loss\n",
        "\n",
        "  if iteration % 500 == 0:\n",
        "    print(iteration, loss)\n",
        "  erro = y_proba - y_train_encoded\n",
        "  gradients = 1/m * X_train.T.dot(erro) + np.r_[np.zeros([1,n_outputs]), alpha * thetas[1:]]\n",
        "  thetas = thetas - eta * gradients\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 135.1887482273817\n",
            "500 1.1065121409072043\n",
            "1000 1.1009987297865185\n",
            "1500 1.1009982548262804\n",
            "2000 1.1009980150644496\n",
            "2500 1.1009977753154552\n",
            "3000 1.1009975355692303\n",
            "3500 1.1009972958257743\n",
            "4000 1.1009970560850877\n",
            "4500 1.10099681634717\n",
            "5000 1.1009965766120213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_m7nU4g3lU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3af05304-464d-49f4-f134-7083f2b57cd5"
      },
      "source": [
        "logits = X_val.dot(thetas)\n",
        "y_proba = Softmax(logits)\n",
        "prediction = np.argmax(y_proba, axis=1) #Returns the INDEX with the highest probability\n",
        "y_val_int = [int(string_val) for string_val in y_val ]\n",
        "score_accuracy = np.mean(prediction == y_val_int)\n",
        "score_accuracy\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8598000465008138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    }
  ]
}